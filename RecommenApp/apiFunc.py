import csv
import os
import pickle
import re
import requests
import json
import pandas as pd

import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
import wordcloud
import csv

from pyvi import ViTokenizer

# ----------------------------------------- #
# ----- start - dá»± Ä‘oÃ¡n vÃ  phÃ¢n tÃ­ch pháº£n há»“i ------#
def _assign_reviews(reviews):
    # LÆ°u dá»¯ liá»‡u vÃ o file CSV
    save_to_csv(reviews, 'tiki.csv')
    print('2 Ä‘Ã£ lwu')


def get_prod_id(url):
    prod_id = url.split('-p')[-1].split('.html')[0]
    return prod_id


def save_to_csv(data, filename, file_path):
    # Kiá»ƒm tra vÃ  táº¡o má»›i file náº¿u chÆ°a tá»“n táº¡i
    if not os.path.exists(file_path):
        with open(file_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            # writer.writerow(["Review"])  # Ghi header náº¿u cáº§n thiáº¿t

    try:
        # Má»Ÿ file Ä‘á»ƒ ghi dá»¯ liá»‡u
        with open(os.path.join(file_path, filename), 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)

            # Náº¿u file Ä‘Ã£ tá»“n táº¡i vÃ  cÃ³ dá»¯ liá»‡u, xÃ³a ná»™i dung cÅ©
            if os.path.exists(os.path.join(file_path, filename)) and os.stat(
                    os.path.join(file_path, filename)).st_size > 0:
                f.seek(0)
                f.truncate()

            # Ghi dá»¯ liá»‡u má»›i vÃ o file CSV
            writer.writerow(["content"])
            for review in data:
                writer.writerow([review])

        print(f"LÆ°u dá»¯ liá»‡u vÃ o file {filename} thÃ nh cÃ´ng trong thÆ° má»¥c {file_path}.")
    except Exception as e:
        print(f"Lá»—i khi lÆ°u dá»¯ liá»‡u vÃ o file {filename}: {e}")
def get_directory(folder):
    directory = os.path.join(os.getcwd(), folder)
    if not os.path.exists(directory):
        os.makedirs(directory)
    return directory


_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
    'X-Guest-Token': '5wy6mHsTRpjA0Z89PaCcVSDLOxNegEtf',
    'Accept': 'application/json, text/plain, */*',
    'Accept-Encoding': 'gzip, deflate, br, zstd',
    'Accept-Language': 'vi',
}

# _PARAMS = {
#           'limit': '{limit}',
#           'include': 'comments,contribute_info,attribute_vote_summary',
#           'sort': f'stars|{star}',
#           'page': {page},
#           'product_id': {product_id},
# }

_COOKIES = {
    # 'TIKI_RECOMMENDATION':'c2b76cc59f1d63ef5aaa9e9883de3fdc',
    'TKSESSID': '16f55fcbd35a5cdb883de25536242ee4',
    'TOKENS': '{%22access_token%22:%225wy6mHsTRpjA0Z89PaCcVSDLOxNegEtf%22%2C%22expires_in%22:157680000%2C%22expires_at%22:1870359973526%2C%22guest_token%22:%225wy6mHsTRpjA0Z89PaCcVSDLOxNegEtf%22}',
    'delivery_zone': 'Vk4wMzQwMjQwMTM=',
    '_trackity': '7e5dcab4-fa5f-57c7-59c0-7a428c155263'
}

# thá»±c hiá»‡n láº¥y vÃ  lÆ°u dá»¯ liá»‡u
def get_reviews(prod_id, spid):
    reviews,reviews_count = get_all_reviews(prod_id, spid)
    # lÆ°u dá»¯ liá»‡u thÃ nh file csv
    rev_df = pd.DataFrame(reviews,columns=['content'])
    file_path = os.path.join(get_directory('data'), f'{prod_id}_origin_reviews.csv')
    rev_df.to_csv(file_path, index=False)
    # save_to_csv(reviews, f'${prod_id}_origin_reviews.csv', 'data')
    return reviews_count


def get_all_reviews(product_id, spid):
    all_reviews = []

    limit = 20  # Giá»›i háº¡n sá»‘ lÆ°á»£ng Ä‘Ã¡nh giÃ¡ má»—i láº§n gá»i
    headers = _HEADERS
    cookies = _COOKIES

    page = 1
    while True:
        # TIKI_REVIEWS_API = f'https://tiki.vn/api/v2/reviews?include=comments,contribute_info,attribute_vote_summary&sort=stars%7C{stars}&page={page}&product_id={product_id}&limit={limit}'
        TIKI_REVIEWS_API = f'https://tiki.vn/api/v2/reviews?limit=20&include=comments,contribute_info,attribute_vote_summary&sort=score%7Cdesc,id%7Cdesc,stars%7Call&page={page}&spid={spid}&product_id={product_id}&seller_id=1'
        print(TIKI_REVIEWS_API)
        response = requests.get(TIKI_REVIEWS_API, headers=headers, cookies=cookies)
        res_body = json.loads(response.content)

        reviews_count = res_body.get('reviews_count', 0)
        # dá»«ng náº¿u khÃ´ng cÃ³ reviews
        if reviews_count == 0:
            print('Not has comment!')
            break

        last_page = res_body['paging']['last_page']
        reviews = res_body['data']

        print('loading...')

        # thÃªm cÃ¡c Ä‘Ã¡nh giÃ¡ vÃ o all_reviews
        for review in reviews:
            if review['content'].strip():
                all_reviews.append(review['content'])
            else:
                if review['rating'] == 5:
                    all_reviews.append("Cá»±c kÃ¬ hÃ i lÃ²ng")
                elif review['rating'] == 4:
                    all_reviews.append("HÃ i lÃ²ng")
                elif review['rating'] == 3:
                    all_reviews.append("BÃ¬nh thÆ°á»ng")
                elif review['rating'] == 2:
                    all_reviews.append("KhÃ´ng hÃ i lÃ²ng")
                elif review['rating'] == 1:
                    all_reviews.append("Ráº¥t khÃ´ng hÃ i lÃ²ng")

        page += 1
        if page > last_page:
            break

    return all_reviews,reviews_count


def get_product_info(product_id):
    product_info_api = f'https://tiki.vn/api/v2/products/{product_id}?platform=web&spid=97551647&version=3'
    headers = _HEADERS
    cookies = _COOKIES

    response = requests.get(product_info_api.format(product_id), headers=headers, cookies=cookies)
    res_body = json.loads(response.content)
    info = {
        'name' : res_body['name'],
        'rating_average' : res_body['rating_average'],
        'review_count' : res_body['review_count']
    }
    return info


# ---- end - thá»±c hiá»‡n láº¥y vÃ  lÆ°u dá»¯ liá»‡u -----#

# ------------- chuáº©n hÃ³a dá»¯ liá»‡u --------------
from pyvi import ViTokenizer
import re


# ------- chuáº©n hÃ³a dá»¯ liá»‡u
def standardize_data(row):
    if isinstance(row, str):
        #Remove all . , " ... in sentences
        row = row.replace(",", " ").replace(".", " ") \
            .replace(";", " ").replace("â€œ", " ") \
            .replace(":", " ").replace("â€", " ") \
            .replace('"', " ").replace("'", " ") \
            .replace("!", " ").replace("?", " ") \
            .replace("-", " ").replace("?", " ") \
            .replace(" ğŸ»", " ")
        row = re.sub(r'\s+', ' ', row)

        #Loáº¡i bá» cÃ¡c tá»« kÃ©o dÃ i nhÆ° Ä‘áº¹ppppp -> Ä‘áº¹p
        row = re.sub(r'([A-Z])\1+', lambda m: m.group(1).upper(), row, flags=re.IGNORECASE)

        #Lowercase
        row = row.lower()

        #Chuáº©n hÃ³a tiáº¿ng Viá»‡t, xá»­ lÃ½ emoj, chuáº©n hÃ³a tiáº¿ng Anh, thuáº­t ngá»¯
        replace_list = {
            'Ã²a': 'oÃ ', 'Ã³a': 'oÃ¡', 'á»a': 'oáº£', 'Ãµa': 'oÃ£', 'á»a': 'oáº¡', 'Ã²e': 'oÃ¨', 'Ã³e': 'oÃ©', 'á»e': 'oáº»',
            'Ãµe': 'oáº½', 'á»e': 'oáº¹', 'Ã¹y': 'uá»³', 'Ãºy': 'uÃ½', 'á»§y': 'uá»·', 'Å©y': 'uá»¹', 'á»¥y': 'uá»µ', 'uáº£': 'á»§a',
            'aÌ‰': 'áº£', 'Ã´Ì': 'á»‘', 'uÂ´': 'á»‘', 'Ã´Ìƒ': 'á»—', 'Ã´Ì€': 'á»“', 'Ã´Ì‰': 'á»•', 'Ã¢Ì': 'áº¥', 'Ã¢Ìƒ': 'áº«', 'Ã¢Ì‰': 'áº©',
            'Ã¢Ì€': 'áº§', 'oÌ‰': 'á»', 'ÃªÌ€': 'á»', 'ÃªÌƒ': 'á»…', 'ÄƒÌ': 'áº¯', 'uÌ‰': 'á»§', 'ÃªÌ': 'áº¿', 'Æ¡Ì‰': 'á»Ÿ', 'iÌ‰': 'á»‰',
            'eÌ‰': 'áº»', 'Ã k': u' Ã  ', 'aË‹': 'Ã ', 'iË‹': 'Ã¬', 'ÄƒÂ´': 'áº¯', 'Æ°Ì‰': 'á»­', 'eËœ': 'áº½', 'yËœ': 'á»¹', 'aÂ´': 'Ã¡',
            'ak': 'Ã ',
            #Quy cÃ¡c icon vá» 3 loáº¡i emoj: TÃ­ch cá»±c tiÃªu cá»±c vÃ  trung lÃ¢p
            "ğŸ‘»": "positive", "ğŸ’ƒ": "positive", 'ğŸ¤™': ' positive ', 'ğŸ‘': ' positive ', 'ğŸ¦¾': 'positive',
            "ğŸ’„": "neutral", "ğŸ’": "neutral", "ğŸ’©": "positive", "ğŸ˜•": "negative", "ğŸ˜±": "negative", "ğŸ˜¸": "positive",
            "ğŸ˜¾": "negative", "ğŸš«": "negative", "ğŸ¤¬": "negative", "ğŸ§š": "neutral", "ğŸ§¡": "positive", 'ğŸ¶': ' neuutral ',
            'ğŸ‘': ' negative ', 'ğŸ˜£': ' negative ', 'âœ¨': ' positive ', 'â£': ' positive ', 'â˜€': ' positive ',
            'â™¥': ' positive ', 'ğŸ¤©': ' positive ', 'like': ' positive ', 'ğŸ’Œ': ' neutral ', 'ğŸ¥°': 'positive',
            'ğŸ¤£': ' positive ', 'ğŸ–¤': ' positive ', 'ğŸ¤¤': ' positive ', ':(': ' negative ', 'ğŸ˜¢': ' negative ',
            'â¤': ' positive ', 'ğŸ˜': ' positive ', 'ğŸ˜˜': ' positive ', 'ğŸ˜ª': ' negative ', 'ğŸ˜Š': ' positive ',
            '?': ' ? ', 'ğŸ˜': ' positive ', 'ğŸ’–': ' positive ', 'ğŸ˜Ÿ': ' negative ', 'ğŸ˜­': ' negative ',
            'ğŸ’¯': ' positive ', 'ğŸ’—': ' positive ', 'â™¡': ' positive ', 'ğŸ’œ': ' positive ', 'ğŸ¤—': ' positive ',
            '^^': ' positive ', 'ğŸ˜¨': ' negative ', 'â˜º': ' positive ', 'ğŸ’‹': ' positive ', 'ğŸ‘Œ': ' positive ',
            'ğŸ˜–': ' negative ', 'ğŸ˜€': ' positive ', ':((': ' negative ', 'ğŸ˜¡': ' negative ', 'ğŸ˜ ': ' negative ',
            'ğŸ˜’': ' negative ', 'ğŸ™‚': ' posiive ', 'ğŸ˜': ' negative ', 'ğŸ˜': ' positive ', 'ğŸ˜„': ' positive ',
            'ğŸ˜™': ' positive ', 'ğŸ˜¤': ' negative ', 'ğŸ˜': ' positive ', 'ğŸ˜†': ' positive ', 'ğŸ’š': ' positive ',
            'âœŒ': ' positive ', 'ğŸ’•': ' positive ', 'ğŸ˜': ' negative ', 'ğŸ˜“': ' negative ', 'ï¸ğŸ†—ï¸': ' positive ',
            'ğŸ˜‰': ' positive ', 'ğŸ˜‚': ' positive ', ':v': '  positive ', '=))': '  positive ', 'ğŸ˜‹': ' positive ',
            'ğŸ’“': ' positive ', 'ğŸ˜': ' neutral ', ':3': ' positive ', 'ğŸ˜«': ' negative ', 'ğŸ˜¥': ' negative ',
            'ğŸ˜ƒ': ' positive ', 'ğŸ˜¬': ' ğŸ˜¬ ', 'ğŸ˜Œ': ' ğŸ˜Œ ', 'ğŸ’›': ' positive ', 'ğŸ¤': ' positive ', 'ğŸˆ': ' positive ',
            'ğŸ˜—': ' positive ', 'ğŸ¤”': ' negative ', 'ğŸ˜‘': ' negative ', 'ğŸ”¥': ' negative ', 'ğŸ™': ' negative ',
            'ğŸ†—': ' positive ', 'ğŸ˜»': ' positive ', 'ğŸ’™': ' positive ', 'ğŸ’Ÿ': ' positive ',
            'ğŸ˜š': ' positive ', 'âŒ': ' negative ', 'ğŸ‘': ' positive ', ';)': ' positive ', '<3': ' positive ',
            'ğŸŒ': ' positive ', 'ğŸŒ·': ' positive ', 'ğŸŒ¸': ' positive ', 'ğŸŒº': ' positive ',
            'ğŸŒ¼': ' neutral ', 'ğŸ“': ' positive ', 'ğŸ…': ' positive ', 'ğŸ¾': ' positive ', 'ğŸ‘‰': ' positive ',
            'ğŸ’': ' positive ', 'ğŸ’': ' positive ', 'ğŸ’¥': ' positive ', 'ğŸ’ª': ' positive ', 'ğŸ˜˜': 'positive',
            'ğŸ’°': ' positive ', 'ğŸ˜‡': ' positive ', 'ğŸ˜›': ' positive ', 'ğŸ˜œ': ' positive ', 'ğŸ‘': 'positive',
            'ğŸ™ƒ': ' positive ', 'ğŸ¤‘': ' positive ', 'ğŸ¤ª': ' positive ', 'â˜¹': ' negative ', 'ğŸ’€': ' negative ',
            'ğŸ˜”': ' negative ', 'ğŸ˜§': ' neutral ', 'ğŸ˜©': ' negative ', 'ğŸ˜°': ' negative ', 'ğŸ˜³': ' neutral ',
            'ğŸ˜µ': ' neutral', 'ğŸ˜¶': ' neutral ', 'ğŸ™': ' neutral ', 'ğŸ¥³': 'positive', 'ğŸ˜…': 'neutural',

            #Chuáº©n hÃ³a 1 sá»‘ sentiment words/English words
            ':))': '  positive ', ':)': ' positive ', 'Ã´ kÃªi': ' ok ', 'okie': ' ok ', ' o kÃª ': ' ok ',
            'yc': 'yÃªu cáº§u',
            'okey': ' ok ', 'Ã´kÃª': ' ok ', 'oki': ' ok ', ' oke ': ' ok ', ' okay': ' ok ', 'okÃª': ' ok ',
            ' tks ': u' cÃ¡m Æ¡n ', 'thks': u' cÃ¡m Æ¡n ', 'thanks': u' cÃ¡m Æ¡n ', 'ths': u' cÃ¡m Æ¡n ', 'thank': u' cÃ¡m Æ¡n ',
            'â­': 'star ', '*': 'star ', 'ğŸŒŸ': 'star ', 'ğŸ‰': u' positive ', ' ng ': ' ngÆ°á»i ',
            'not': u' khÃ´ng ', ' kh ': u' khÃ´ng ', 'kÃ´': u' khÃ´ng ', 'hok': u' khÃ´ng ', ' kp ': u' khÃ´ng pháº£i ',
            u' kÃ´ ': u' khÃ´ng ', u' k ': u' khÃ´ng ', '"ko ': u' khÃ´ng ', u' ko ': u' khÃ´ng ', 'khong': u' khÃ´ng ',
            u' hok ': u' khÃ´ng ',
            'he he': ' positive ', 'hehe': ' positive ', 'hihi': ' positive ', 'haha': ' positive ',
            'hjhj': ' positive ',
            ' lol ': ' negative ', ' cc ': ' negative ', 'cute': u' dá»… thÆ°Æ¡ng ', 'huhu': ' negative ', ' vs ': u' vá»›i ',
            'wa': ' quÃ¡ ', 'wÃ¡': u' quÃ¡', 'j': u' gÃ¬ ', 'â€œ': ' ',
            ' sz ': u' cá»¡ ', 'size': u' cá»¡ ', u' Ä‘x ': u' Ä‘Æ°á»£c ', 'dk': u' Ä‘Æ°á»£c ', 'dc': u' Ä‘Æ°á»£c ', 'Ä‘k': u' Ä‘Æ°á»£c ',
            'Ä‘uoc': u'Ä‘Æ°á»£c', 'mn': u'má»i ngÆ°á»i',
            'Ä‘c': u' Ä‘Æ°á»£c ', 'authentic': u' chuáº©n chÃ­nh hÃ£ng ', u' aut ': u' chuáº©n chÃ­nh hÃ£ng ',
            u' auth ': u' chuáº©n chÃ­nh hÃ£ng ', 'thick': u' positive ', 'store': u' cá»­a hÃ ng ',
            'shop': u' cá»­a hÃ ng ', 'sp': u' sáº£n pháº©m ', 'gud': u' tá»‘t ', 'god': u' tá»‘t ', 'wel done': ' tá»‘t ',
            'good': u' tá»‘t ', 'gÃºt': u' tá»‘t ',
            'sáº¥u': u' xáº¥u ', 'gut': u' tá»‘t ', u' tot ': u' tá»‘t ', u' nice ': u' tá»‘t ', 'perfect': 'ráº¥t tá»‘t',
            'bt': u' bÃ¬nh thÆ°á»ng ',
            'time': u' thá»i gian ', 'qÃ¡': u' quÃ¡ ', u' ship ': u' giao hÃ ng ', u' m ': u' mÃ¬nh ', u' mik ': u' mÃ¬nh ',
            'ÃªÌ‰': 'á»ƒ', 'product': 'sáº£n pháº©m', 'quality': 'cháº¥t lÆ°á»£ng', 'chat': ' cháº¥t ', 'excelent': 'hoÃ n háº£o',
            'bad': 'tá»‡', 'fresh': ' tÆ°Æ¡i ', 'sad': ' tá»‡ ',
            'date': u' háº¡n sá»­ dá»¥ng ', 'hsd': u' háº¡n sá»­ dá»¥ng ', 'quickly': u' nhanh ', 'quick': u' nhanh ',
            'fast': u' nhanh ', 'delivery': u' giao hÃ ng ', u' sÃ­p ': u' giao hÃ ng ',
            'beautiful': u' Ä‘áº¹p tuyá»‡t vá»i ', u' tl ': u' tráº£ lá»i ', u' r ': u' rá»“i ', u' shopE ': u' cá»­a hÃ ng ',
            u' order ': u' Ä‘áº·t hÃ ng ',
            'cháº¥t lg': u' cháº¥t lÆ°á»£ng ', u' sd ': u' sá»­ dá»¥ng ', u' dt ': u' Ä‘iá»‡n thoáº¡i ', u' nt ': u' nháº¯n tin ',
            u' tl ': u' tráº£ lá»i ', u' sÃ i ': u' xÃ i ', u'bjo': u' bao giá» ',
            'thik': u' thÃ­ch ', u' sop ': u' cá»­a hÃ ng ', ' fb ': ' facebook ', ' face ': ' facebook ',
            ' very ': u' ráº¥t ', u'quáº£ ng ': u' quáº£ng  ',
            'dep': u' Ä‘áº¹p ', u' xau ': u' xáº¥u ', 'delicious': u' ngon ', u'hÃ g': u' hÃ ng ', u'qá»§a': u' quáº£ ',
            u' dá»… máº·t ': u' dá»… máº·c ', u' tháº¥y vá»ng ': u' tháº¥t vá»ng ', u' vá»g ': u' vá»ng ',
            'iu': u' yÃªu ', 'fake': u' giáº£ máº¡o ', 'trl': 'tráº£ lá»i', '><': u' positive ', 'nice': u' tá»‘t ',
            u' rat ': u' ráº¥t ', u' ung ': u' Æ°ng ', u' ung ho ': u' á»§ng há»™ ',
            ' por ': u' tá»‡ ', ' poor ': u' tá»‡ ', 'ib': u' nháº¯n tin ', 'rep': u' tráº£ lá»i ', u'fback': ' feedback ',
            'fedback': ' feedback ', u' bill ': u' hÃ³a Ä‘Æ¡n ', u' chuan ': u' chuáº©n ',
            u' chuáº£n ': u' chuáº©n ', u' cx ': u' cÅ©ng ', u' cg ': u' cÅ©ng ', u' cÅ©g ': u' cÅ©ng ', u' cugx ': u' cÅ©ng ',
            u' thá»a mÃ¡i ': u' thoáº£i mÃ¡i ', u' thoai mai ': u' thoáº£i mÃ¡i ',
            u' ap ': u' á»©ng dá»¥ng ', u' app ': u' á»©ng dá»¥ng ', u' sÃ i ': u' xÃ i ', u' tÃµ rÃ ng ': u'rÃµ rÃ ng',
            #quy chuan sao thanh star
            '5 sao': ' 5star ', '5 star': ' 5star ', '5sao': ' 5star ', '4 star': ' 4star ', '4 sao': ' 4star ',
            '3sao': ' 3star ', '3 star': ' 3star ', '3 sao': ' 3star ', '3sao': ' 3star ',
            'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ', '2 sao': ' 2star ',
            '2sao': ' 2star ',
            '2 starstar': ' 2star ', '1star': ' 1star ', }

        for k, v in replace_list.items():
            row = row.replace(k, v)

        row = row.strip()
        return row
    else:
        return ''

# tÃ¡ch cÃ¢u thÃ nh cÃ¡c tá»« riÃªng láº»
def tokenizer(row):
    if isinstance(row, str):
        return ViTokenizer.tokenize(row)
    else:
        return ''

def predict_process_data(data):
    # Táº¡o DataFrame tá»« dá»¯ liá»‡u vÃ  Ä‘áº·t tÃªn cá»™t lÃ  'content'
    data_frame = pd.DataFrame(data, columns=['content'])

    # Loáº¡i bá» cÃ¡c dÃ²ng cÃ³ giÃ¡ trá»‹ NaN trong cá»™t 'content'
    data_frame = data_frame.dropna(subset=['content'])

    # Ãp dá»¥ng tiÃªu chuáº©n hÃ³a dá»¯ liá»‡u
    data_frame['content'] = data_frame['content'].apply(standardize_data)

    # Ãp dá»¥ng tokenizer vÃ o dá»¯ liá»‡u
    data_frame['content'] = data_frame['content'].apply(tokenizer)

    return data_frame

# ---- end - chuáº©n hÃ³a dá»¯ liá»‡u -----------

# def processing_data(data):
#     #Standardize data
#     data_frame = pd.DataFrame(data)
#     #print('dataframe':, data_frame)
#     data_frame[0] = data_frame[0].apply(standardize_data)
#
#     #Tokenizer
#     data_frame[0] = data_frame[0].apply(tokenizer)
#     return data_frame[0]

# --------- xá»­ lÃ½ dá»¯ liá»‡u Ä‘á»ƒ tiáº¿n hÃ nh dá»± Ä‘oÃ¡n

# load mÃ´ hÃ¬nh TF-IDF
# Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã³, giÃºp biáº¿n Ä‘á»•i cÃ¡c vÄƒn báº£n thÃ nh dáº¡ng Ä‘áº·c trÆ°ng
# sá»‘ Ä‘á»ƒ cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¡c thuáº­t toÃ¡n há»c mÃ¡y cho cÃ¡c tÃ¡c vá»¥ nhÆ° phÃ¢n loáº¡i, há»“i quy, v.v.
# ÄÆ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i tá»›i file tfidf_vectorizer.pk
tf_file_path = os.path.join(os.path.dirname(__file__), 'model', 'tfidf_vectorizer.pk')
#Load TF-IDF weight
tf_idf = pickle.load(open(tf_file_path, 'rb'))

# ------ dá»± Ä‘oÃ¡n -------------
def predict(prod_id):
    #Load URL and print comments
    try:
        file_path = os.path.join(get_directory('data'), f'{prod_id}_origin_reviews.csv')
        data = pd.read_csv(file_path)
    except Exception as e:
        print(e)
        return
    data = predict_process_data(data)

    # # predict
    sav_file_path = os.path.join(os.path.dirname(__file__), 'model', 'naive_bayes_model.sav')
    pl = pickle.load(open(sav_file_path, "rb"))
    X_test_tfidf = tf_idf.transform(data['content'])
    print(X_test_tfidf.shape)
    y_pred = pl.predict(X_test_tfidf)

    cols = ['content', 'label']
    processed_data = pd.DataFrame(columns=cols)
    processed_data['label'] = y_pred
    processed_data['content'] = data
    # return processed_data
    return processed_data
def get_label_counts(processed_data):
    # PhÃ¢n loáº¡i label thÃ nh cÃ¡c nhÃ³m tÆ°Æ¡ng á»©ng
    processed_data['label_group'] = processed_data['label'].apply(lambda x: 'Positive' if x == 5.0 else ('Negative' if x in [1.0, 2.0] else 'Neutral'))

    # Äáº¿m sá»‘ lÆ°á»£ng tá»«ng nhÃ³m
    label_counts = processed_data['label_group'].value_counts()
    print(label_counts)

    pos_count = label_counts.get('Positive', 0)
    neg_count = label_counts.get('Negative', 0)
    neu_count = label_counts.get('Neutral', 0)

    return pos_count, neg_count, neu_count
def print_label_counts(pos_count, neg_count, neu_count):
    print("Positive reviews  ğŸ˜Š: ", pos_count)
    print("Negative reviews  ğŸ˜¡: ", neg_count)
    print("Neutral reviews   ğŸ˜: ", neu_count)
#  váº½ pie_chart
def draw_pie_chart(pos_count, neg_count, neu_count):
    pie_chart = np.array([pos_count, neg_count, neu_count])
    mylabels = ["Positive", "Negative", "Neutral"]

    if np.sum(pie_chart) == 0:
        print("No data to create pie chart.")
    else:
        # plt.figure(figsize=(10, 7))  # TÄƒng kÃ­ch thÆ°á»›c cá»§a figure
        plt.pie(pie_chart, labels=mylabels, shadow=True, startangle=90, autopct='%1.1f%%')
        plt.tight_layout()  # Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh bá»‘ cá»¥c Ä‘á»ƒ giáº£m khoáº£ng tráº¯ng thá»«a

        img_path = os.path.join('img', 'pie_chart.png')  # ÄÆ°á»ng dáº«n Ä‘áº§y Ä‘á»§ cá»§a thÆ° má»¥c img
        plt.savefig(img_path)
        plt.close()
    return img_path

# Ä‘Ã¡nh giÃ¡ - rating - sá»‘ â­
def predict_rating(pos_count, neg_count, neu_count):
    rating = float((pos_count * 5 + neg_count * 1.5 + neu_count * 3.5) / (pos_count + neg_count + neu_count))
    return round(rating, 1)

def print_rating_suggestion(rating):
    print("Rating Predict: ", rating, "â­")
    if rating > 4:
        print("Good! You can buy it! ğŸ‘ŒğŸ‘ŒğŸ‘Œ")
    elif rating > 2 and rating <= 4:
        print("Please check it carefully! â—â—â—")
    elif rating <= 2:
        print("Bad! ğŸ‘ğŸ‘ğŸ‘")
    print("--------------------------------------")
# ---- end - dá»± Ä‘oÃ¡n ---------------

# --- start - phÃ¢n tÃ­ch
def analyze(processed_data):
    label_counts = get_label_counts(processed_data)
    pos_count, neg_count, neu_count = label_counts

    rating = predict_rating(pos_count, neg_count, neu_count)
    print_rating_suggestion(rating)

    create_word_clouds(processed_data)


wc_stop_ws = [u'ráº±ng', u'thÃ¬', u'lÃ ', u'mÃ ', u'mÃ¬nh', u'bÃ©', u'cho', u'chá»‰', u'em', u'vÃ ', u'gÃ¬', u'nÃ y', u'cÃ¡c',
               u'cÃ¡i', u'cáº§n', u'cÃ ng', u'cho', u'chiáº¿c', u'chá»©', u'ng', u'cÃ¹ng', u'cÅ©ng',
               u'gÃ¬', u'cá»§a', u'cÃ²n', u'cÃ³', u'cÅ©ng', u'khi', u'sáº½', u'vá»', u'vá»›i', u'Ä‘á»ƒ', u'nÃªn', u'thÃªm', u'con',
               u'ná»¯a', u'sau', u'so', u'tÃ´i', u'vÃ¬', u'váº«n', u'Ä‘Ã£', u'láº§n', u'láº¡i', u'báº±ng', u'biáº¿t',
               u'do', u'ngÆ°á»i', u'ra', u'tiki', u'váº­y', u'trÃªn', u'trong', u'giá»‘ng', u'cáº£', u'bÃªn', u'giá»', u'nÃ o',
               u'rá»“i', u'Ä‘áº§u', u'bá»™', u'má»™t', u'nhÆ°', u'Ä‘i', u'Ä‘Æ¡n', u'Ä‘Ã¢y',
               u'vá»«a', u'Ä‘Ã³', u'dc', u'dÃ¹ng', u'ghi', u'hÆ¡i', u'hÆ¡n', u'há»™p', u'loáº¡i', u'luÃ´n', u'lÃ m', u'lÃªn',
               u'lÃºc', u'láº¯m', u'mÃ¡y', u'máº«u', u'má»Ÿ', u'ngoÃ i', u'ngÃ y', u'nhiá»u', u'nhÃ ', u'nhÃ¬n', u'nÃ³i',
               u'nÆ°á»›c', u'pháº£i', u'qua', u'sao', u'tháº¥y', u'trÆ°á»›c', u'tá»›i', u'tá»«', u'vÃ o', u'vá»«a', u'xem', u'xong',
               u'báº¡n', u'chÃ­nh', u'chÆ¡i', u'hang', u'má»i', u'má»›i', u'nhÃ©', u'quÃ¡',
               u'táº§m', u'ai', u'báº¡n', u'bÃ¡o', u'dÃ¡n', u'kg', u'miáº¿ng', u'máº¥y', u'máº·c', u'nÃ³', u'pháº§n', u'thÃ¡ng',
               u'tiáº¿ng', u'Ä‘áº¿n', u'xe', u'bh', u'cÃ¡o']
def create_word_clouds(test_data):
    # neg_word = test_data['content'].loc[test_data['label'] == 2]
    # pos_word = test_data['content'].loc[test_data['label'] == 1]
    # neu_word = test_data['content'].loc[test_data['label'] == 3]
    pos_word = test_data['content'].loc[(test_data['label'] >= 4) & (test_data['label'] <= 5)]
    neg_word = test_data['content'].loc[(test_data['label'] >= 1) & (test_data['label'] <= 2)]
    neu_word = test_data['content'].loc[test_data['label'] == 3]

    # Debugging print statements
    print("Positive words:")
    print(pos_word)
    print("Negative words:")
    print(neg_word)
    print("Neutral words:")
    print(neu_word)

    tfidf_pos = TfidfVectorizer(ngram_range=(1, 5), min_df=0.01, max_df=0.99, stop_words=wc_stop_ws, max_features=100,
                                sublinear_tf=True, smooth_idf=True)
    tfidf_neg = TfidfVectorizer(ngram_range=(1, 5), min_df=0.01, max_df=0.99, stop_words=wc_stop_ws, max_features=100,
                                sublinear_tf=True, smooth_idf=True)
    tfidf_neu = TfidfVectorizer(ngram_range=(1, 5), min_df=0.01, max_df=0.99, stop_words=wc_stop_ws, max_features=100,
                                sublinear_tf=True, smooth_idf=True)

    tfidf_matrix_pos = create_tfidf_matrix(tfidf_pos, pos_word)
    tfidf_matrix_neg = create_tfidf_matrix(tfidf_neg, neg_word)
    tfidf_matrix_neu = create_tfidf_matrix(tfidf_neu, neu_word)

    feature_names_pos = tfidf_pos.get_feature_names_out() if tfidf_matrix_pos is not None else []
    feature_names_neg = tfidf_neg.get_feature_names_out() if tfidf_matrix_neg is not None else []
    feature_names_neu = tfidf_neu.get_feature_names_out() if tfidf_matrix_neu is not None else []

    create_word_cloud(feature_names_pos, "Positive")
    create_word_cloud(feature_names_neg, "Negative")
    create_word_cloud(feature_names_neu, "Neutral")


def create_tfidf_matrix(vectorizer, word_list):
    if word_list.empty:
        return None
    else:
        return vectorizer.fit_transform(word_list)


def save_feature_names_to_txt(feature_names, file_name):
    file_path = os.path.join('analyze', file_name+'.txt')
    try:
        with open(file_path, 'w', encoding='utf-8') as file:
            for feature in feature_names:
                file.write(f"{feature}\n")
        print(f"Feature names saved to {file_path}")
    except Exception as e:
        print(f"An error occurred while saving feature names: {e}")

def create_word_cloud(feature_names, sentiment):
    print(f"{sentiment} Review :")
    # print('cwc feature_names: ',feature_names)
    save_feature_names_to_txt(feature_names, f'{sentiment} Reviews')
    if len(feature_names) > 0:
        # cloud = np.array(feature_names)
        cloud = " ".join(feature_names)
        plt.figure(figsize=(12, 6))
        plt.title(f"WordCloud for {sentiment} Reviews")
        word_cloud = (wordcloud.WordCloud(max_words=30, background_color="black",
                                         width=1200, height=600, mode="RGB").generate(str(cloud)))
        plt.axis("off")
        plt.imshow(word_cloud)

        img_path = os.path.join('img',f'{sentiment}_word_cloud.png')
        plt.savefig(img_path)  # LÆ°u hÃ¬nh áº£nh vÃ o thÆ° má»¥c 'img'
        print(f"{sentiment} WordCloud saved at {img_path}")
    print("--------------------------------------")


# ----- end - dá»± Ä‘oÃ¡n vÃ  phÃ¢n tÃ­ch pháº£n há»“i ------#
# ----------------------------------------- #
