import os
import pickle
import re
import requests
import json
import pandas as pd

import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
import wordcloud

from pyvi import ViTokenizer

class PredictHand:
    def __init__(self, ui):
        self.ui = ui
    def get_all_reviews(self, product_id):
        all_reviews = []

        limit = 10  # Giá»›i háº¡n sá»‘ lÆ°á»£ng Ä‘Ã¡nh giÃ¡ má»—i láº§n gá»i
        stars = 5

        while stars <= 5:
          page = 1
          reviews = []
          TIKI_REVIEWS_API = f'https://tiki.vn/api/v2/reviews?include=comments,contribute_info,attribute_vote_summary&sort=stars%7C{stars}&page={page}&product_id={product_id}&limit={limit}'

          headers = {
              'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
              'X-Guest-Token':'5wy6mHsTRpjA0Z89PaCcVSDLOxNegEtf',
              'Accept':'application/json, text/plain, */*',
              'Accept-Encoding':'gzip, deflate, br, zstd',
              'Accept-Language':'vi',
          }

          params ={
              'limit': limit,
              'include': 'comments,contribute_info,attribute_vote_summary',
              'sort': f'stars|{stars}',
              'page': page,
              'product_id': {product_id},
          }

          cookies = {
              # 'TIKI_RECOMMENDATION':'c2b76cc59f1d63ef5aaa9e9883de3fdc',
              'TKSESSID': '16f55fcbd35a5cdb883de25536242ee4',
              'TOKENS' : '{%22access_token%22:%225wy6mHsTRpjA0Z89PaCcVSDLOxNegEtf%22%2C%22expires_in%22:157680000%2C%22expires_at%22:1870359973526%2C%22guest_token%22:%225wy6mHsTRpjA0Z89PaCcVSDLOxNegEtf%22}',
              'delivery_zone' : 'Vk4wMzQwMjQwMTM=',
              '_trackity' : '7e5dcab4-fa5f-57c7-59c0-7a428c155263'
          }

          response = requests.get(TIKI_REVIEWS_API.format(stars,page,product_id,limit),headers = headers, params = params, cookies = cookies)
          res_body = json.loads(response.content)

          reviews_count = res_body['reviews_count']
          # dá»«ng náº¿u khÃ´ng cÃ³ reviews
          if reviews_count == 0:
              print('Not has comment!')
              break

          last_page = res_body['paging']['last_page']
          reviews = reviews + res_body['data']

          while True:
            # thÃªm cÃ¡c Ä‘Ã¡nh giÃ¡ vÃ o all_reviews
            for review in reviews:
              if (review['content'] != "" or review['content'].strip()):
                all_reviews += [review['content']]
              elif (review['content'] == "" and review['rating'] == 5 ):
                all_reviews += ["Cá»±c kÃ¬ hÃ i lÃ²ng"]
              elif (review['content'] == "" and review['rating'] == 4 ):
                all_reviews += ["HÃ i lÃ²ng"]
              elif (review['content'] == "" and review['rating'] == 3 ):
                all_reviews += ["BÃ¬nh thÆ°á»ng"]
              elif (review['content'] == "" and review['rating'] == 2 ):
                all_reviews += ["KhÃ´ng hÃ i lÃ²ng"]
              elif (review['content'] == "" and review['rating'] == 1 ):
                all_reviews += ["Ráº¥t khÃ´ng hÃ i lÃ²ng"]

            page+=1

            if(page>last_page):
                break

          stars-=1
          if(stars == 0):
            break

        return all_reviews

    # ------------- chuáº©n hÃ³a dá»¯ liá»‡u --------------
    from pyvi import ViTokenizer
    import re
    # chuáº©n hÃ³a dá»¯ liá»‡u
    def standardize_data(self, row):
      if isinstance(row, str):
        #Remove all . , " ... in sentences
        row = row.replace(",", " ").replace(".", " ") \
              .replace(";", " ").replace("â€œ", " ") \
              .replace(":", " ").replace("â€", " ") \
              .replace('"', " ").replace("'", " ") \
              .replace("!", " ").replace("?", " ") \
              .replace("-", " ").replace("?", " ")\
              .replace(" ğŸ»"," ")
        row = re.sub(r'\s+', ' ', row)

        #Loáº¡i bá» cÃ¡c tá»« kÃ©o dÃ i nhÆ° Ä‘áº¹ppppp -> Ä‘áº¹p
        row = re.sub(r'([A-Z])\1+', lambda m: m.group(1).upper(), row, flags=re.IGNORECASE)

        #Lowercase
        row = row.lower()

        #Chuáº©n hÃ³a tiáº¿ng Viá»‡t, xá»­ lÃ½ emoj, chuáº©n hÃ³a tiáº¿ng Anh, thuáº­t ngá»¯
        replace_list = {
            'Ã²a': 'oÃ ', 'Ã³a': 'oÃ¡', 'á»a': 'oáº£', 'Ãµa': 'oÃ£', 'á»a': 'oáº¡', 'Ã²e': 'oÃ¨', 'Ã³e': 'oÃ©','á»e': 'oáº»',
            'Ãµe': 'oáº½', 'á»e': 'oáº¹', 'Ã¹y': 'uá»³', 'Ãºy': 'uÃ½', 'á»§y': 'uá»·', 'Å©y': 'uá»¹','á»¥y': 'uá»µ', 'uáº£': 'á»§a',
            'aÌ‰': 'áº£', 'Ã´Ì': 'á»‘', 'uÂ´': 'á»‘','Ã´Ìƒ': 'á»—', 'Ã´Ì€': 'á»“', 'Ã´Ì‰': 'á»•', 'Ã¢Ì': 'áº¥', 'Ã¢Ìƒ': 'áº«', 'Ã¢Ì‰': 'áº©',
            'Ã¢Ì€': 'áº§', 'oÌ‰': 'á»', 'ÃªÌ€': 'á»','ÃªÌƒ': 'á»…', 'ÄƒÌ': 'áº¯', 'uÌ‰': 'á»§', 'ÃªÌ': 'áº¿', 'Æ¡Ì‰': 'á»Ÿ', 'iÌ‰': 'á»‰',
            'eÌ‰': 'áº»', 'Ã k': u' Ã  ','aË‹': 'Ã ', 'iË‹': 'Ã¬', 'ÄƒÂ´': 'áº¯','Æ°Ì‰': 'á»­', 'eËœ': 'áº½', 'yËœ': 'á»¹', 'aÂ´': 'Ã¡',
            'ak': 'Ã ',
            #Quy cÃ¡c icon vá» 3 loáº¡i emoj: TÃ­ch cá»±c tiÃªu cá»±c vÃ  trung lÃ¢p
            "ğŸ‘»": "positive", "ğŸ’ƒ": "positive",'ğŸ¤™': ' positive ', 'ğŸ‘': ' positive ','ğŸ¦¾':'positive',
            "ğŸ’„": "neutral", "ğŸ’": "neutral", "ğŸ’©": "positive","ğŸ˜•": "negative", "ğŸ˜±": "negative", "ğŸ˜¸": "positive",
            "ğŸ˜¾": "negative", "ğŸš«": "negative",  "ğŸ¤¬": "negative","ğŸ§š": "neutral", "ğŸ§¡": "positive",'ğŸ¶':' neuutral ',
            'ğŸ‘': ' negative ', 'ğŸ˜£': ' negative ','âœ¨': ' positive ', 'â£': ' positive ','â˜€': ' positive ',
            'â™¥': ' positive ', 'ğŸ¤©': ' positive ', 'like': ' positive ', 'ğŸ’Œ': ' neutral ','ğŸ¥°':'positive',
            'ğŸ¤£': ' positive ', 'ğŸ–¤': ' positive ', 'ğŸ¤¤': ' positive ', ':(': ' negative ', 'ğŸ˜¢': ' negative ',
            'â¤': ' positive ', 'ğŸ˜': ' positive ', 'ğŸ˜˜': ' positive ', 'ğŸ˜ª': ' negative ', 'ğŸ˜Š': ' positive ',
            '?': ' ? ', 'ğŸ˜': ' positive ', 'ğŸ’–': ' positive ', 'ğŸ˜Ÿ': ' negative ', 'ğŸ˜­': ' negative ',
            'ğŸ’¯': ' positive ', 'ğŸ’—': ' positive ', 'â™¡': ' positive ', 'ğŸ’œ': ' positive ', 'ğŸ¤—': ' positive ',
            '^^': ' positive ', 'ğŸ˜¨': ' negative ', 'â˜º': ' positive ', 'ğŸ’‹': ' positive ', 'ğŸ‘Œ': ' positive ',
            'ğŸ˜–': ' negative ', 'ğŸ˜€': ' positive ', ':((': ' negative ', 'ğŸ˜¡': ' negative ', 'ğŸ˜ ': ' negative ',
            'ğŸ˜’': ' negative ', 'ğŸ™‚': ' posiive ', 'ğŸ˜': ' negative ', 'ğŸ˜': ' positive ', 'ğŸ˜„': ' positive ',
            'ğŸ˜™': ' positive ', 'ğŸ˜¤': ' negative ', 'ğŸ˜': ' positive ', 'ğŸ˜†': ' positive ', 'ğŸ’š': ' positive ',
            'âœŒ': ' positive ', 'ğŸ’•': ' positive ', 'ğŸ˜': ' negative ', 'ğŸ˜“': ' negative ', 'ï¸ğŸ†—ï¸': ' positive ',
            'ğŸ˜‰': ' positive ', 'ğŸ˜‚': ' positive ', ':v': '  positive ', '=))': '  positive ', 'ğŸ˜‹': ' positive ',
            'ğŸ’“': ' positive ', 'ğŸ˜': ' neutral ', ':3': ' positive ', 'ğŸ˜«': ' negative ', 'ğŸ˜¥': ' negative ',
            'ğŸ˜ƒ': ' positive ', 'ğŸ˜¬': ' ğŸ˜¬ ', 'ğŸ˜Œ': ' ğŸ˜Œ ', 'ğŸ’›': ' positive ', 'ğŸ¤': ' positive ', 'ğŸˆ': ' positive ',
            'ğŸ˜—': ' positive ', 'ğŸ¤”': ' negative ', 'ğŸ˜‘': ' negative ', 'ğŸ”¥': ' negative ', 'ğŸ™': ' negative ',
            'ğŸ†—': ' positive ', 'ğŸ˜»': ' positive ', 'ğŸ’™': ' positive ', 'ğŸ’Ÿ': ' positive ',
            'ğŸ˜š': ' positive ', 'âŒ': ' negative ', 'ğŸ‘': ' positive ', ';)': ' positive ', '<3': ' positive ',
            'ğŸŒ': ' positive ',  'ğŸŒ·': ' positive ', 'ğŸŒ¸': ' positive ', 'ğŸŒº': ' positive ',
            'ğŸŒ¼': ' neutral ', 'ğŸ“': ' positive ', 'ğŸ…': ' positive ', 'ğŸ¾': ' positive ', 'ğŸ‘‰': ' positive ',
            'ğŸ’': ' positive ', 'ğŸ’': ' positive ', 'ğŸ’¥': ' positive ', 'ğŸ’ª': ' positive ', 'ğŸ˜˜':'positive',
            'ğŸ’°': ' positive ',  'ğŸ˜‡': ' positive ', 'ğŸ˜›': ' positive ', 'ğŸ˜œ': ' positive ','ğŸ‘':'positive',
            'ğŸ™ƒ': ' positive ', 'ğŸ¤‘': ' positive ', 'ğŸ¤ª': ' positive ','â˜¹': ' negative ',  'ğŸ’€': ' negative ',
            'ğŸ˜”': ' negative ', 'ğŸ˜§': ' neutral ', 'ğŸ˜©': ' negative ', 'ğŸ˜°': ' negative ', 'ğŸ˜³': ' neutral ',
            'ğŸ˜µ': ' neutral', 'ğŸ˜¶': ' neutral ', 'ğŸ™': ' neutral ', 'ğŸ¥³':'positive','ğŸ˜…': 'neutural',

            #Chuáº©n hÃ³a 1 sá»‘ sentiment words/English words
            ':))': '  positive ', ':)': ' positive ', 'Ã´ kÃªi': ' ok ', 'okie': ' ok ', ' o kÃª ': ' ok ','yc':'yÃªu cáº§u',
            'okey': ' ok ', 'Ã´kÃª': ' ok ', 'oki': ' ok ', ' oke ':  ' ok ',' okay':' ok ','okÃª':' ok ',
            ' tks ': u' cÃ¡m Æ¡n ', 'thks': u' cÃ¡m Æ¡n ', 'thanks': u' cÃ¡m Æ¡n ', 'ths': u' cÃ¡m Æ¡n ', 'thank': u' cÃ¡m Æ¡n ',
            'â­': 'star ', '*': 'star ', 'ğŸŒŸ': 'star ', 'ğŸ‰': u' positive ', ' ng ' :' ngÆ°á»i ',
            'not': u' khÃ´ng ',' kh ':u' khÃ´ng ','kÃ´':u' khÃ´ng ','hok':u' khÃ´ng ',' kp ': u' khÃ´ng pháº£i ',u' kÃ´ ': u' khÃ´ng ', u' k ': u' khÃ´ng ', '"ko ': u' khÃ´ng ', u' ko ': u' khÃ´ng ','khong': u' khÃ´ng ', u' hok ': u' khÃ´ng ',
            'he he': ' positive ','hehe': ' positive ','hihi': ' positive ', 'haha': ' positive ', 'hjhj': ' positive ',
            ' lol ': ' negative ',' cc ': ' negative ','cute': u' dá»… thÆ°Æ¡ng ','huhu': ' negative ', ' vs ': u' vá»›i ', 'wa': ' quÃ¡ ', 'wÃ¡': u' quÃ¡', 'j': u' gÃ¬ ', 'â€œ': ' ',
            ' sz ': u' cá»¡ ', 'size': u' cá»¡ ', u' Ä‘x ': u' Ä‘Æ°á»£c ', 'dk': u' Ä‘Æ°á»£c ', 'dc': u' Ä‘Æ°á»£c ', 'Ä‘k': u' Ä‘Æ°á»£c ', 'Ä‘uoc': u'Ä‘Æ°á»£c', 'mn': u'má»i ngÆ°á»i',
            'Ä‘c': u' Ä‘Æ°á»£c ','authentic': u' chuáº©n chÃ­nh hÃ£ng ',u' aut ': u' chuáº©n chÃ­nh hÃ£ng ', u' auth ': u' chuáº©n chÃ­nh hÃ£ng ', 'thick': u' positive ', 'store': u' cá»­a hÃ ng ',
            'shop': u' cá»­a hÃ ng ', 'sp': u' sáº£n pháº©m ', 'gud': u' tá»‘t ','god': u' tá»‘t ','wel done':' tá»‘t ', 'good': u' tá»‘t ', 'gÃºt': u' tá»‘t ',
            'sáº¥u': u' xáº¥u ','gut': u' tá»‘t ', u' tot ': u' tá»‘t ', u' nice ': u' tá»‘t ', 'perfect': 'ráº¥t tá»‘t', 'bt': u' bÃ¬nh thÆ°á»ng ',
            'time': u' thá»i gian ', 'qÃ¡': u' quÃ¡ ', u' ship ': u' giao hÃ ng ', u' m ': u' mÃ¬nh ', u' mik ': u' mÃ¬nh ',
            'ÃªÌ‰': 'á»ƒ', 'product': 'sáº£n pháº©m', 'quality': 'cháº¥t lÆ°á»£ng','chat':' cháº¥t ', 'excelent': 'hoÃ n háº£o', 'bad': 'tá»‡','fresh': ' tÆ°Æ¡i ','sad': ' tá»‡ ',
            'date': u' háº¡n sá»­ dá»¥ng ', 'hsd': u' háº¡n sá»­ dá»¥ng ','quickly': u' nhanh ', 'quick': u' nhanh ','fast': u' nhanh ','delivery': u' giao hÃ ng ',u' sÃ­p ': u' giao hÃ ng ',
            'beautiful': u' Ä‘áº¹p tuyá»‡t vá»i ', u' tl ': u' tráº£ lá»i ', u' r ': u' rá»“i ', u' shopE ': u' cá»­a hÃ ng ',u' order ': u' Ä‘áº·t hÃ ng ',
            'cháº¥t lg': u' cháº¥t lÆ°á»£ng ',u' sd ': u' sá»­ dá»¥ng ',u' dt ': u' Ä‘iá»‡n thoáº¡i ',u' nt ': u' nháº¯n tin ',u' tl ': u' tráº£ lá»i ',u' sÃ i ': u' xÃ i ',u'bjo':u' bao giá» ',
            'thik': u' thÃ­ch ',u' sop ': u' cá»­a hÃ ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' ráº¥t ',u'quáº£ ng ':u' quáº£ng  ',
            'dep': u' Ä‘áº¹p ',u' xau ': u' xáº¥u ','delicious': u' ngon ', u'hÃ g': u' hÃ ng ', u'qá»§a': u' quáº£ ', u' dá»… máº·t ': u' dá»… máº·c ',u' tháº¥y vá»ng ': u' tháº¥t vá»ng ',u' vá»g ': u' vá»ng ',
            'iu': u' yÃªu ','fake': u' giáº£ máº¡o ', 'trl': 'tráº£ lá»i', '><': u' positive ','nice' : u' tá»‘t ',u' rat ': u' ráº¥t ', u' ung ': u' Æ°ng ', u' ung ho ': u' á»§ng há»™ ',
            ' por ': u' tá»‡ ',' poor ': u' tá»‡ ', 'ib':u' nháº¯n tin ', 'rep':u' tráº£ lá»i ',u'fback':' feedback ','fedback':' feedback ',u' bill ': u' hÃ³a Ä‘Æ¡n ', u' chuan ': u' chuáº©n ',
            u' chuáº£n ': u' chuáº©n ',u' cx ': u' cÅ©ng ',u' cg ': u' cÅ©ng ', u' cÅ©g ': u' cÅ©ng ', u' cugx ': u' cÅ©ng ', u' thá»a mÃ¡i ': u' thoáº£i mÃ¡i ', u' thoai mai ': u' thoáº£i mÃ¡i ',
            u' ap ': u' á»©ng dá»¥ng ', u' app ': u' á»©ng dá»¥ng ', u' sÃ i ': u' xÃ i ', u' tÃµ rÃ ng ': u'rÃµ rÃ ng',
            #quy chuan sao thanh star
            '5 sao': ' 5star ','5 star': ' 5star ','5sao': ' 5star ', '4 star': ' 4star ','4 sao': ' 4star ','3sao': ' 3star ','3 star': ' 3star ','3 sao': ' 3star ','3sao': ' 3star ',
            'starstarstarstarstar': ' 5star ', '1 sao': ' 1star ', '1sao': ' 1star ','2 sao':' 2star ','2sao':' 2star ',
            '2 starstar':' 2star ','1star': ' 1star ',}

        for k, v in replace_list.items():
            row = row.replace(k, v)

        row = row.strip()
        return row
      else:
          return ''

    # tÃ¡ch cÃ¢u thÃ nh cÃ¡c tá»« riÃªng láº»
    def tokenizer(self, row):
        if isinstance(row, str):
            return ViTokenizer.tokenize(row)
        else:
            return ''

    def processing_data(self, data):
        #Standardize data
        data_frame = pd.DataFrame(data)
        #print('dataframe':, data_frame)
        data_frame[0] = data_frame[0].apply(self.standardize_data)

        #Tokenizer
        data_frame[0] = data_frame[0].apply(self.tokenizer)
        return data_frame[0]

    #  dá»± Ä‘oÃ¡n
    def predict(self, prod_id):
        #Load URL and print comments
        data = self.get_all_reviews(prod_id)
        data = self.processing_data(data)

        #predict
        sav_file_path = os.path.join(os.path.dirname(__file__), 'model','naive_bayes_model.sav')
        pl = pickle.load(open(sav_file_path, "rb"))
        X_test_tfidf = tf_idf.transform(data)
        print(X_test_tfidf.shape)
        y_pred = pl.predict(X_test_tfidf)

        cols = ['content', 'label']
        test_data = pd.DataFrame(columns=cols)
        test_data['label'] = y_pred
        test_data['content'] = data
        return test_data

    # phÃ¢n tÃ­ch
    def analyze(self,test_data):
        label_counts = self.get_label_counts(test_data)
        pos_count, neg_count, neu_count = label_counts
        self.print_label_counts(pos_count, neg_count, neu_count)

        self.draw_pie_chart(pos_count, neg_count, neu_count)

        rating = self.predict_rating(pos_count, neg_count, neu_count)
        self.print_rating_suggestion(rating)

        self.create_word_clouds(test_data)

    def get_label_counts(self, test_data):
        label_counts = test_data['label'].value_counts()
        pos_count = label_counts.get(1, 0)
        neg_count = label_counts.get(2, 0)
        neu_count = label_counts.get(3, 0)
        return pos_count, neg_count, neu_count

    def print_label_counts(self,pos_count, neg_count, neu_count):
        print("Positive reviews  ğŸ˜Š: ", pos_count)
        print("Negative reviews  ğŸ˜¡: ", neg_count)
        print("Neutral reviews   ğŸ˜: ", neu_count)

    def draw_pie_chart(self,pos_count, neg_count, neu_count):
        pie_chart = np.array([pos_count, neg_count, neu_count])
        mylabels = ["Positive", "Negative", "Neutral"]

        if np.sum(pie_chart) == 0:
            print("No data to create pie chart.")
        else:
            plt.pie(pie_chart, labels=mylabels, shadow=True, startangle=90, autopct='%1.1f%%')
            plt.show()
        print("--------------------------------------")

    def predict_rating(self,pos_count, neg_count, neu_count):
        rating = float((pos_count * 5 + neg_count * 1.5 + neu_count * 3.5) / (pos_count + neg_count + neu_count))
        return round(rating, 1)

    def print_rating_suggestion(self,rating):
        print("Rating Predict: ", rating, "â­")
        if rating > 4:
            print("Good! You can buy it! ğŸ‘ŒğŸ‘ŒğŸ‘Œ")
        elif rating > 2 and rating <= 4:
            print("Please check it carefully! â—â—â—")
        elif rating <= 2:
            print("Bad! ğŸ‘ğŸ‘ğŸ‘")
        print("--------------------------------------")

    def create_word_clouds(self,test_data):
        stop_ws = [u'ráº±ng', u'thÃ¬', u'lÃ ', u'mÃ ', u'mÃ¬nh', u'bÃ©', u'cho', u'chá»‰', u'em', u'vÃ ', u'gÃ¬', u'nÃ y', u'cÃ¡c',
                   u'cÃ¡i', u'cáº§n', u'cÃ ng', u'cho', u'chiáº¿c', u'chá»©', u'ng', u'cÃ¹ng', u'cÅ©ng',
                   u'gÃ¬', u'cá»§a', u'cÃ²n', u'cÃ³', u'cÅ©ng', u'khi', u'sáº½', u'vá»', u'vá»›i', u'Ä‘á»ƒ', u'nÃªn', u'thÃªm', u'con',
                   u'ná»¯a', u'sau', u'so', u'tÃ´i', u'vÃ¬', u'váº«n', u'Ä‘Ã£', u'láº§n', u'láº¡i', u'báº±ng', u'biáº¿t',
                   u'do', u'ngÆ°á»i', u'ra', u'tiki', u'váº­y', u'trÃªn', u'trong', u'giá»‘ng', u'cáº£', u'bÃªn', u'giá»', u'nÃ o',
                   u'rá»“i', u'Ä‘áº§u', u'bá»™', u'má»™t', u'nhÆ°', u'Ä‘i', u'Ä‘Æ¡n', u'Ä‘Ã¢y',
                   u'vá»«a', u'Ä‘Ã³', u'dc', u'dÃ¹ng', u'ghi', u'hÆ¡i', u'hÆ¡n', u'há»™p', u'loáº¡i', u'luÃ´n', u'lÃ m', u'lÃªn',
                   u'lÃºc', u'láº¯m', u'mÃ¡y', u'máº«u', u'má»Ÿ', u'ngoÃ i', u'ngÃ y', u'nhiá»u', u'nhÃ ', u'nhÃ¬n', u'nÃ³i',
                   u'nÆ°á»›c', u'pháº£i', u'qua', u'sao', u'tháº¥y', u'trÆ°á»›c', u'tá»›i', u'tá»«', u'vÃ o', u'vá»«a', u'xem', u'xong',
                   u'báº¡n', u'chÃ­nh', u'chÆ¡i', u'hang', u'má»i', u'má»›i', u'nhÃ©', u'quÃ¡',
                   u'táº§m', u'ai', u'báº¡n', u'bÃ¡o', u'dÃ¡n', u'kg', u'miáº¿ng', u'máº¥y', u'máº·c', u'nÃ³', u'pháº§n', u'thÃ¡ng',
                   u'tiáº¿ng', u'Ä‘áº¿n', u'xe', u'bh', u'cÃ¡o']

        neg_word = test_data['content'].loc[test_data['label'] == 2]
        pos_word = test_data['content'].loc[test_data['label'] == 1]
        neu_word = test_data['content'].loc[test_data['label'] == 3]

        tfidf_pos = TfidfVectorizer(ngram_range=(1, 5), min_df=4, max_df=0.9, stop_words=stop_ws, max_features=100,
                                    sublinear_tf=True, smooth_idf=True)
        tfidf_neg = TfidfVectorizer(ngram_range=(1, 5), min_df=2, max_df=0.9, stop_words=stop_ws, max_features=100,
                                    sublinear_tf=True, smooth_idf=True)
        tfidf_neu = TfidfVectorizer(ngram_range=(1, 5), min_df=3, max_df=0.9, stop_words=stop_ws, max_features=100,
                                    sublinear_tf=True, smooth_idf=True)

        tfidf_matrix_pos = self.create_tfidf_matrix(tfidf_pos, pos_word)
        tfidf_matrix_neg = self.create_tfidf_matrix(tfidf_neg, neg_word)
        tfidf_matrix_neu = self.create_tfidf_matrix(tfidf_neu, neu_word)

        feature_names_pos = tfidf_pos.get_feature_names_out() if tfidf_matrix_pos is not None else []
        feature_names_neg = tfidf_neg.get_feature_names_out() if tfidf_matrix_neg is not None else []
        feature_names_neu = tfidf_neu.get_feature_names_out() if tfidf_matrix_neu is not None else []

        self.create_word_cloud(feature_names_pos, "Positive")
        self.create_word_cloud(feature_names_neg, "Negative")
        self.create_word_cloud(feature_names_neu, "Neutral")

    def create_tfidf_matrix(self,vectorizer, word_list):
        if word_list.empty:
            return None
        else:
            return vectorizer.fit_transform(word_list)

    def create_word_cloud(self,feature_names, sentiment):
        print(f"{sentiment} Review :")
        print(feature_names)
        if feature_names:
            cloud = np.array(feature_names)
            plt.figure(figsize=(12, 6))
            plt.title(f"WordCloud for {sentiment} Reviews")
            word_cloud = wordcloud.WordCloud(max_words=30, background_color="black",
                                             width=1200, height=600, mode="RGB").generate(str(cloud))
            plt.axis("off")
            plt.imshow(word_cloud)
        print("--------------------------------------")

# load mÃ´ hÃ¬nh TF-IDF
# Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c Ä‘Ã³, giÃºp biáº¿n Ä‘á»•i cÃ¡c vÄƒn báº£n thÃ nh dáº¡ng Ä‘áº·c trÆ°ng
# sá»‘ Ä‘á»ƒ cÃ³ thá»ƒ Ã¡p dá»¥ng cÃ¡c thuáº­t toÃ¡n há»c mÃ¡y cho cÃ¡c tÃ¡c vá»¥ nhÆ° phÃ¢n loáº¡i, há»“i quy, v.v.
# ÄÆ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i tá»›i file tfidf_vectorizer.pk
tf_file_path = os.path.join(os.path.dirname(__file__), 'model', 'tfidf_vectorizer.pk')
#Load TF-IDF weight
tf_idf = pickle.load(open(tf_file_path, 'rb'))